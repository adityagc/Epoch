import ts.flint
from ts.flint import FlintContext
from pyspark import SparkContext
from pyspark import SQLContext
from pyspark.sql import SparkSession
from datetime import date
from ts.flint import summarizers
import psycopg2

#Creating Spark, SQL and Flint contexts:
spark = SparkSession.builder.appName("ts").getOrCreate()
sqlContext = SQLContext(spark)
flintContext = FlintContext(sqlContext)

#Reading dataframes
#Automate this using boto

#Reading files manually:
stocks = ['ABT', 'ACN', 'AOS', 'ATVI', 'ABBV']
bucket_name = 's3a://insightde2018bucket/'
folder_name = bucket_name + '10-5-2018/'
#Creating reference table:
stock = stocks[0]
all_returns = spark.read.option('header', True).option('inferSchema', True).csv(folder_name + stock + '.csv'). \
    withColumnRenamed('date', 'time').withColumnRenamed('1. open', 'open').withColumnRenamed('4. close', 'close'). \
    withColumnRenamed('2. high', 'high').withColumnRenamed('3. low', 'low').withColumnRenamed('5. volume', 'volume')
all_returns = flintContext.read.dataframe(all_returns)
all_returns = all_returns.select('time')
print(all_returns.show())

#Iterating through all stocks:
for stock in stocks:
	flint_stock = spark.read.option('header', True).option('inferSchema', True).csv(folder_name + stock + '.csv'). \
	withColumnRenamed('date', 'time').withColumnRenamed('1. open', 'open').withColumnRenamed('4. close', 'close'). \
	withColumnRenamed('2. high', 'high').withColumnRenamed('3. low', 'low').withColumnRenamed('5. volume', 'volume')
	flint_stock = flintContext.read.dataframe(flint_stock)
	stock_return = flint_stock.withColumn( stock , 100 * (flint_stock['close'] - flint_stock['open']) / flint_stock['open']).select('time', stock)
	all_returns = all_returns.futureLeftJoin(stock_return, key='time', tolerance = '120s')
	#print(stock_return.show())

#Display table for reference:
print(all_returns.show())

#Summarizing correlation coefficients:
corr = all_returns.summarize(summarizers.correlation(stocks[0], other = stocks[1:]))

jdbcHostname = "timeseries.c07kxyw4xlcx.us-east-1.rds.amazonaws.com"
jdbcDatabase = ""
jdbcPort = 5432
jdbcUrl = "jdbc:mysql://{0}:{1}/{2}?".format(jdbcHostname, jdbcPort, jdbcDatabase)

jdbcUrl = "jdbc:mysql://{0}:{1}/{2}".format(jdbcHostname, jdbcPort, jdbcDatabase)
connectionProperties = {
  "user" : jdbcUsername,
  "password" : jdbcPassword,
  "driver" : "com.mysql.jdbc.Driver"
}

pushdown_query = "(select * from employees where emp_no < 10008) emp_alias"
df = spark.read.jdbc(url=jdbcUrl, dbtable=pushdown_query, properties=connectionProperties)
display(df)

